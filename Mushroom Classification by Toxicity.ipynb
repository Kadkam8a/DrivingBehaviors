{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3edc08de",
   "metadata": {},
   "source": [
    "- Karime Ochoa Jacinto.\n",
    "- Anton Pashkov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94222d6f",
   "metadata": {},
   "source": [
    "**_Abstract._ This article describes a machine learning project to predict the toxicity of hypothetical mushroom samples based on their physical properties using Naive Bayes and Decision Trees.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b390a0",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82186313",
   "metadata": {},
   "source": [
    "Collecting mushrooms (shrooming) is a popular hobby for many people; however, it is a rather dangerous activity even for the most experienced collectors because of the potential toxicity coupled with nature's mechanisms that cause that non-venomous species emulate venomous features as a protection mechanism, a phenomenon known as mimicry. As such, this project seeks to construct a reliable prediction model of mushrooms' toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64cc200",
   "metadata": {},
   "source": [
    "## Materials and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0508550b",
   "metadata": {},
   "source": [
    "The model was implemented in the Python programming language (version 3.8), as it provides all the necessary libraries for data analysis and machine learning: Pandas, Numpy, Matplotlib, and Scikit-learn. Prior to the final version of the model, several algorithms and parameters were tested, each of which are documented in this article. The mushroom dataset used in this project was downloaded from Kaggle (https://www.kaggle.com/datasets/uciml/mushroom-classification). It contains information on the color and shape of different mushrooms' caps and gills, as well as their bruises and odors. Each mushroom is classified as poisonous (p) or edible (e)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db0135e",
   "metadata": {},
   "source": [
    "## Experiments and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835f7e44",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a8c22",
   "metadata": {},
   "source": [
    "The first step was to import the dataset as a Pandas DataFrame (**Table 1**). The import required no special parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f043e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"mushrooms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678928d",
   "metadata": {},
   "source": [
    "| |class|cap-shape|cap-surface|cap-color|bruises|\n",
    "|--:|--:|--:|--:|--:|--:|\n",
    "|**0**|p|x|s|n|t|\n",
    "|**1**|e|x|s|y|t|\n",
    "|**2**|e|b|s|w|t|\n",
    "|**3**|p|x|y|w|t|\n",
    "|**4**|e|x|s|g|f|\n",
    "\n",
    "_**Table 1**. A glimpse into the dataset, showing the labels and some of the features available. In total, the table contains 23 columns (including the class labels) and 8124 rows._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d505f",
   "metadata": {},
   "source": [
    "The dataset required no further adjustments, as it contains no missing values or non-sense information. Nevertheless, in order to apply Scikit-learn's machine learning algorithms, the labels had to be encoded to numbers (**Table 2**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c790d72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['p', 'e'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = data[\"class\"].unique()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "data = data.apply(LabelEncoder().fit_transform)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b40ac",
   "metadata": {},
   "source": [
    "| |class|cap-shape|cap-surface|cap-color|bruises|\n",
    "|--:|--:|--:|--:|--:|--:|\n",
    "|**0**|1|5|2|4|1|\n",
    "|**1**|0|5|2|9|1|\n",
    "|**2**|0|0|2|w|1|\n",
    "|**3**|1|5|3|8|1|\n",
    "|**4**|0|5|2|3|0|\n",
    "\n",
    "_**Table 2**. The same section as Table 1, after apply label encoding; notice how the \"poisonous\" labels where changed to 1 and the \"non-poisonous\" (edible) ones to 0._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e86103",
   "metadata": {},
   "source": [
    "The dataset was then divided into a series of class labels and the corresponding features associated to each instance, an obligatory requirement for Scikit-learn's algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bde2ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = data[\"class\"]\n",
    "data.drop(\"class\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e934a4c",
   "metadata": {},
   "source": [
    "Finally, the data was split into training and test sets, with each set containing two thirds and one third of the data, respectively. A random state of zero was used to guarantee reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2718a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "split = train_test_split(data, classes, test_size=1/3, random_state=0)\n",
    "x_train, x_test, y_train, y_test = split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa3157",
   "metadata": {},
   "source": [
    "### KN-Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96094c6",
   "metadata": {},
   "source": [
    "The first machine learning model chosen was k-nearest neighbor because we thougt that possibly for the mimicry described in the introduction this would not obtain a fulfilling score. For this model we set k = 90 as it is a good aproximation to the square of the total number of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb172c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      1420\n",
      "           1       0.99      0.92      0.95      1288\n",
      "\n",
      "    accuracy                           0.96      2708\n",
      "   macro avg       0.96      0.95      0.96      2708\n",
      "weighted avg       0.96      0.96      0.96      2708\n",
      "\n",
      "0.9524576954069299\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "neighbors = int(np.sqrt(len(classes)))\n",
    "knn = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "knn.fit(x_train, y_train)\n",
    "pred = knn.predict(x_test)\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "score = f1_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e40f9",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e1cbf",
   "metadata": {},
   "source": [
    "The second machine learning model applied were decision trees. The huge benefit of using trees is that they provide a visual output, useful for non-technical people interested in the data. Different parameters were tested in order to find the most appropiate configuration for the given data. Each setup was evaluated through five-fold cross-validation using $F_1$ as the scoring metric, and calculating its mean. Just like with the splitting, a random state of zero was employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f33b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as dtc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "criteria = [\"entropy\", \"gini\"]\n",
    "depths = range(2, 10)\n",
    "results = pd.DataFrame(\n",
    "    columns=[\"criterion\", \"depth\", \"score\"],\n",
    "    index=range(len(criteria) * len(depths))\n",
    ")\n",
    "\n",
    "i = 0\n",
    "for c in criteria:\n",
    "    for d in depths:\n",
    "        model = dtc(random_state=0, criterion=c, max_depth=d)\n",
    "        scores = cross_val_score(model, x_train, y_train, scoring=\"f1\")\n",
    "        results.iloc[i] = [c, d, scores.mean()]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9a548",
   "metadata": {},
   "source": [
    "| |criterion|depth|score|\n",
    "|--:|--:|--:|--:|\n",
    "|**5**|entropy|7|1|\n",
    "|**6**|entropy|8|1|\n",
    "|**7**|entropy|9|1|\n",
    "|**14**|gini|8|1|\n",
    "|**15**|gini|9|1|\n",
    "\n",
    "_**Table 3**. Several configurations create a decision tree with a $F_1$ score of 1._\n",
    "\n",
    "Because there were many configurations producing great results (**Table 3**), each of the models was applied on the test data to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df47872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "final_scores = results[results.score == 1]\n",
    "\n",
    "for i in final_scores.index:\n",
    "    c = final_scores.loc[i][\"criterion\"]\n",
    "    d = final_scores.loc[i][\"depth\"]\n",
    "    model = dtc(random_state=0, criterion=c, max_depth=d)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    score = f1_score(y_test, y_pred)\n",
    "    final_scores.loc[i][\"score\"] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a71c3c9",
   "metadata": {},
   "source": [
    "After making the predictions, the same values as in **Table 3** were obtained. One could suppose that any of the models is adequate; however, building longer trees might bias the predictions due to overfitting. As such, the chosen model uses entropy as the criterion of split quality and a depth of seven. A graphical visualization of this tree can be seen in **Figure 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "model = dtc(random_state=0, criterion=\"entropy\", max_depth=7)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(21, 15))\n",
    "plot_tree(\n",
    "    model,\n",
    "    class_names=[\"poisonous\", \"edible\"],\n",
    "    feature_names=data.columns,\n",
    "    filled=True,\n",
    "    fontsize=10,\n",
    "    ax=ax\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172bca35",
   "metadata": {},
   "source": [
    "_**Figure 1**. The final decision tree constructed for mushroom toxicity prediction._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d16b07",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb984c7",
   "metadata": {},
   "source": [
    "The results of this project might take the readers to the wrong direction regarding the poisonous nature of mushrooms. As mentioned in the introduction, many biological creatures employ mimicry for survival purposes, meaning that it is possible to stumble upon poisonous fungi that look like edible, and viceversa. However, at least for the data presented in the dataset, the results are extremely satisfying, and might be used as a \"general rule of thumb\" when classifying mushrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb75a5",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa585a3",
   "metadata": {},
   "source": [
    "UCI Machine Learning (2017). Mushroom Classification: Safe to eat or deadly poison? Retrieved from https://www.kaggle.com/datasets/uciml/mushroom-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3597f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
